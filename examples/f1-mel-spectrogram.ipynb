{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spoken-object",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:31:20.874688Z",
     "iopub.status.busy": "2021-05-20T10:31:20.874100Z",
     "iopub.status.idle": "2021-05-20T10:31:23.124202Z",
     "shell.execute_reply": "2021-05-20T10:31:23.126176Z",
     "shell.execute_reply.started": "2021-05-20T10:25:32.341282Z"
    },
    "papermill": {
     "duration": 2.275809,
     "end_time": "2021-05-20T10:31:23.126601",
     "exception": false,
     "start_time": "2021-05-20T10:31:20.850792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from functools import lru_cache\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import scipy.fftpack\n",
    "import scipy.linalg\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lyric-wedding",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:31:23.186893Z",
     "iopub.status.busy": "2021-05-20T10:31:23.186022Z",
     "iopub.status.idle": "2021-05-20T10:31:23.200451Z",
     "shell.execute_reply": "2021-05-20T10:31:23.201225Z",
     "shell.execute_reply.started": "2021-05-20T10:25:34.383108Z"
    },
    "id": "MpjhKDYreIQ8",
    "papermill": {
     "duration": 0.046982,
     "end_time": "2021-05-20T10:31:23.201458",
     "exception": false,
     "start_time": "2021-05-20T10:31:23.154476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=10)\n",
    "def get_window(n, type='hamming'):\n",
    "    coefs = np.arange(n)\n",
    "    window = 0.54 - 0.46 * np.cos(2 * np.pi * coefs / (n - 1))\n",
    "    return window\n",
    "\n",
    "def apply_preemphasis(y, preemCoef=0.97):\n",
    "    y[1:] = y[1:] - preemCoef*y[:-1]\n",
    "    y[0] *= (1 - preemCoef)\n",
    "    return y\n",
    "\n",
    "def freq_to_mel(freq):\n",
    "    return 2595.0 * np.log10(1.0 + freq / 700.0)\n",
    "\n",
    "def mel_to_freq(mels):\n",
    "    return 700.0 * (np.power(10.0, mels / 2595.0) - 1.0)\n",
    "\n",
    "@lru_cache(maxsize=10)\n",
    "def get_filterbank(numfilters, filterLen, lowFreq, highFreq, samplingFreq):\n",
    "    minwarpfreq = freq_to_mel(lowFreq)\n",
    "    maxwarpfreq = freq_to_mel(highFreq)\n",
    "    dwarp = (maxwarpfreq - minwarpfreq) / (numfilters + 1)\n",
    "    f = mel_to_freq(np.arange(numfilters + 2) * dwarp + minwarpfreq) * (filterLen - 1) * 2.0 / samplingFreq\n",
    "    i = np.arange(filterLen)[None, :]\n",
    "    f = f[:, None]\n",
    "    hislope = (i - f[:numfilters]) / (f[1:numfilters+1] - f[:numfilters])\n",
    "    loslope = (f[2:numfilters+2] - i) / (f[2:numfilters+2] - f[1:numfilters+1])\n",
    "    H = np.maximum(0, np.minimum(hislope, loslope))\n",
    "    return H\n",
    "\n",
    "def normalized(y, threshold=0):\n",
    "    y -= y.mean()\n",
    "    stddev = y.std()\n",
    "    if stddev > threshold:\n",
    "        y /= stddev\n",
    "    return y\n",
    "\n",
    "def mfsc(y, sfr, window_size=0.025, window_stride=0.010, window='hamming', normalize=True, log=True, n_mels=80, preemCoef=0.97, melfloor=1.0):\n",
    "    win_length = int(sfr * window_size)\n",
    "    hop_length = int(sfr * window_stride)\n",
    "    n_fft = 2048\n",
    "    lowfreq = 0\n",
    "    highfreq = sfr/2\n",
    "    \n",
    "    # get window\n",
    "    window = get_window(win_length)\n",
    "    padded_window = np.pad(window, (0, n_fft - win_length), mode='constant')[:, None]\n",
    "    \n",
    "    # preemphasis\n",
    "    y = apply_preemphasis(y, preemCoef)\n",
    "\n",
    "    # scale wave signal\n",
    "    y *= 32768\n",
    "    \n",
    "    # get frames and scale input\n",
    "    num_frames = 1 + (len(y) - win_length) // hop_length\n",
    "    pad_after = num_frames*hop_length + (n_fft - hop_length) - len(y)\n",
    "    if pad_after > 0:\n",
    "        y = np.pad(y, (0, pad_after), mode='constant')\n",
    "    frames = np.lib.stride_tricks.as_strided(y, shape=(n_fft, num_frames), strides=(y.itemsize, hop_length * y.itemsize), writeable=False)\n",
    "    windowed_frames = padded_window * frames\n",
    "    D = np.abs(np.fft.rfft(windowed_frames, axis=0))\n",
    "\n",
    "    # mel filterbank\n",
    "    filterbank = get_filterbank(n_mels, n_fft/2 + 1, lowfreq, highfreq, sfr)\n",
    "    mf = np.dot(filterbank, D)\n",
    "    mf = np.maximum(melfloor, mf)\n",
    "    if log:\n",
    "        mf = np.log(mf)\n",
    "    if normalize:\n",
    "        mf = normalized(mf)\n",
    "\n",
    "    return mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "isolated-input",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:31:23.254295Z",
     "iopub.status.busy": "2021-05-20T10:31:23.253250Z",
     "iopub.status.idle": "2021-05-20T10:31:23.256878Z",
     "shell.execute_reply": "2021-05-20T10:31:23.257377Z",
     "shell.execute_reply.started": "2021-05-20T10:25:34.403480Z"
    },
    "papermill": {
     "duration": 0.035848,
     "end_time": "2021-05-20T10:31:23.257585",
     "exception": false,
     "start_time": "2021-05-20T10:31:23.221737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_dataset(kaldi_path, class_to_id):\n",
    "    text_path = os.path.join(kaldi_path, 'text')\n",
    "    wav_path = os.path.join(kaldi_path, 'wav.scp')\n",
    "\n",
    "    key_to_word = dict()\n",
    "    key_to_wav = dict()\n",
    "    \n",
    "    with open(wav_path, 'rt') as wav_scp:\n",
    "        for line in wav_scp:\n",
    "            key, wav = line.strip().split(' ', 1)\n",
    "            key_to_wav[key] = wav\n",
    "            key_to_word[key] = None # default\n",
    "\n",
    "    if os.path.isfile(text_path):\n",
    "        with open(text_path, 'rt') as text:\n",
    "            for line in text:\n",
    "                key, word = line.strip().split(' ', 1)\n",
    "                key_to_word[key] = word\n",
    "\n",
    "    wavs = []\n",
    "    for key, wav_command in key_to_wav.items():\n",
    "        word = key_to_word[key]\n",
    "        word_id = class_to_id[word] if word is not None else -1 # default for test\n",
    "        wav_item = [key, wav_command, word_id]\n",
    "        wavs.append(wav_item)\n",
    "\n",
    "    return wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "armed-affairs",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:31:23.299868Z",
     "iopub.status.busy": "2021-05-20T10:31:23.299156Z",
     "iopub.status.idle": "2021-05-20T10:31:23.302363Z",
     "shell.execute_reply": "2021-05-20T10:31:23.301775Z",
     "shell.execute_reply.started": "2021-05-20T10:25:34.417815Z"
    },
    "papermill": {
     "duration": 0.026393,
     "end_time": "2021-05-20T10:31:23.302541",
     "exception": false,
     "start_time": "2021-05-20T10:31:23.276148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wav_read(path):\n",
    "    sr, y = scipy.io.wavfile.read(path)\n",
    "    y = y/32768 # Normalize to -1..1\n",
    "    y -= y.mean()\n",
    "    return y, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prostate-springfield",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:31:23.348294Z",
     "iopub.status.busy": "2021-05-20T10:31:23.347543Z",
     "iopub.status.idle": "2021-05-20T10:31:23.351505Z",
     "shell.execute_reply": "2021-05-20T10:31:23.352544Z",
     "shell.execute_reply.started": "2021-05-20T10:25:34.427603Z"
    },
    "papermill": {
     "duration": 0.03176,
     "end_time": "2021-05-20T10:31:23.352725",
     "exception": false,
     "start_time": "2021-05-20T10:31:23.320965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def param_loader(path, window_size, window_stride, window, normalize, max_len):\n",
    "    y, sfr = wav_read(path)\n",
    "\n",
    "    param = mfsc(y, sfr, window_size=window_size, window_stride=window_stride, window=window, normalize=normalize, log=False, n_mels=60, preemCoef=0, melfloor=1.0)\n",
    "\n",
    "    # Add zero padding to make all param with the same dims\n",
    "    if param.shape[1] < max_len:\n",
    "        pad = np.zeros((param.shape[0], max_len - param.shape[1]))\n",
    "        param = np.hstack((pad, param))\n",
    "\n",
    "    # If exceeds max_len keep last samples\n",
    "    elif param.shape[1] > max_len:\n",
    "        param = param[:, -max_len:]\n",
    "\n",
    "    param = torch.FloatTensor(param)\n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "returning-hungarian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:31:23.398941Z",
     "iopub.status.busy": "2021-05-20T10:31:23.398094Z",
     "iopub.status.idle": "2021-05-20T10:31:23.400270Z",
     "shell.execute_reply": "2021-05-20T10:31:23.399642Z",
     "shell.execute_reply.started": "2021-05-20T10:25:34.438875Z"
    },
    "papermill": {
     "duration": 0.028535,
     "end_time": "2021-05-20T10:31:23.400450",
     "exception": false,
     "start_time": "2021-05-20T10:31:23.371915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_classes():\n",
    "    classes = ['neg', 'pos']\n",
    "    weight = None\n",
    "    class_to_id = {label: i for i, label in enumerate(classes)}\n",
    "    return classes, weight, class_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "latin-bubble",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:31:23.449777Z",
     "iopub.status.busy": "2021-05-20T10:31:23.448949Z",
     "iopub.status.idle": "2021-05-20T10:31:23.454349Z",
     "shell.execute_reply": "2021-05-20T10:31:23.455278Z",
     "shell.execute_reply.started": "2021-05-20T10:25:34.452072Z"
    },
    "papermill": {
     "duration": 0.036558,
     "end_time": "2021-05-20T10:31:23.455560",
     "exception": false,
     "start_time": "2021-05-20T10:31:23.419002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Loader(data.Dataset):\n",
    "    \"\"\"Data set loader::\n",
    "    Args:\n",
    "        root (string): Kaldi directory path.\n",
    "        transform (callable, optional): A function/transform that takes in a spectrogram\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        window_size: window size for the stft, default value is .02\n",
    "        window_stride: window stride for the stft, default value is .01\n",
    "        window_type: typye of window to extract the stft, default value is 'hamming'\n",
    "        normalize: boolean, whether or not to normalize the param to have zero mean and one std\n",
    "        max_len: the maximum length of frames to use\n",
    "     Attributes:\n",
    "        classes (list): List of the class names.\n",
    "        class_to_id (dict): Dict with items (class_name, class_index).\n",
    "        wavs (list): List of (wavs path, class_index) tuples\n",
    "        STFT parameters: window_size, window_stride, window_type, normalize\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None, target_transform=None, window_size=.02,\n",
    "                 window_stride=.01, window_type='hamming', normalize=True, max_len=1000):\n",
    "\n",
    "        classes, weight, class_to_id = get_classes()\n",
    "        self.root = root\n",
    "        self.wavs = make_dataset(root, class_to_id)\n",
    "        self.classes = classes\n",
    "        self.weight = weight\n",
    "        self.class_to_id = class_to_id\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = param_loader\n",
    "        self.window_size = window_size\n",
    "        self.window_stride = window_stride\n",
    "        self.window_type = window_type\n",
    "        self.normalize = normalize\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (key, params, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        key, path, target = self.wavs[index]\n",
    "        path = '../input/covid/wavs16k/' + path\n",
    "        params = self.loader(path, self.window_size, self.window_stride, self.window_type, self.normalize, self.max_len)  # pylint: disable=line-too-long\n",
    "        if self.transform is not None:\n",
    "            params = self.transform(params)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return key, params, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wavs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "attended-legislation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:31:23.512107Z",
     "iopub.status.busy": "2021-05-20T10:31:23.510724Z",
     "iopub.status.idle": "2021-05-20T10:31:23.537982Z",
     "shell.execute_reply": "2021-05-20T10:31:23.538974Z",
     "shell.execute_reply.started": "2021-05-20T10:25:34.466131Z"
    },
    "id": "79opq8kbeIQ9",
    "papermill": {
     "duration": 0.064724,
     "end_time": "2021-05-20T10:31:23.539161",
     "exception": false,
     "start_time": "2021-05-20T10:31:23.474437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, vgg_name, hidden=64, dropout=0.4):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2*512, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.unsqueeze_(1)\n",
    "        x = self.features(x)\n",
    "        x1, _ = x.max(dim=-1)\n",
    "        x2 = x.mean(dim=-1)\n",
    "        x = torch.cat((x1, x2), dim=-1)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=True):\n",
    "    layers = []\n",
    "    in_channels = 1\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "elder-registration",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:31:23.630773Z",
     "iopub.status.busy": "2021-05-20T10:31:23.627748Z",
     "iopub.status.idle": "2021-05-20T10:31:23.648585Z",
     "shell.execute_reply": "2021-05-20T10:31:23.647725Z",
     "shell.execute_reply.started": "2021-05-20T10:25:34.486152Z"
    },
    "papermill": {
     "duration": 0.066281,
     "end_time": "2021-05-20T10:31:23.648794",
     "exception": false,
     "start_time": "2021-05-20T10:31:23.582513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(loader, model, criterion, optimizer, epoch, cuda, log_interval, weight=None, verbose=True):\n",
    "    model.train()\n",
    "    global_epoch_loss = 0\n",
    "    samples = 0\n",
    "    for batch_idx, (_, data, target) in enumerate(loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        global_epoch_loss += loss.data.item() * len(target)\n",
    "        samples += len(target)\n",
    "        if verbose:\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, samples, len(loader.dataset), 100 * samples / len(loader.dataset), global_epoch_loss / samples))\n",
    "    return global_epoch_loss / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "colonial-beauty",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:31:23.715921Z",
     "iopub.status.busy": "2021-05-20T10:31:23.713690Z",
     "iopub.status.idle": "2021-05-20T10:31:23.720310Z",
     "shell.execute_reply": "2021-05-20T10:31:23.720902Z",
     "shell.execute_reply.started": "2021-05-20T10:25:34.498765Z"
    },
    "papermill": {
     "duration": 0.039439,
     "end_time": "2021-05-20T10:31:23.721092",
     "exception": false,
     "start_time": "2021-05-20T10:31:23.681653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(loader, model, criterion, cuda, verbose=True, data_set='Test', save=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    tpred = []\n",
    "    ttarget = []\n",
    "\n",
    "    if save is not None:\n",
    "        csv = open(save, 'wt')\n",
    "        print('index,prob', file=csv)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for keys, data, target in loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            pred = output.sigmoid()\n",
    "            tpred.append(pred.cpu().numpy())\n",
    "\n",
    "            if target[0] != -1:\n",
    "                loss = criterion(output, target.float()).data.item()\n",
    "                test_loss += loss * len(target) # sum up batch loss \n",
    "                ttarget.append(target.cpu().numpy())\n",
    "\n",
    "            if save is not None:\n",
    "                for i, key in enumerate(keys):\n",
    "                    print(f'{key},{pred[i]}', file=csv)\n",
    "    \n",
    "    if len(ttarget) > 0:\n",
    "        test_loss /= len(loader.dataset)\n",
    "        auc = roc_auc_score(np.concatenate(ttarget), np.concatenate(tpred))\n",
    "        if verbose:\n",
    "            print('\\n{} set: Average loss: {:.4f}, AUC: ({:.1f}%)\\n'.format(data_set, test_loss, 100 * auc))\n",
    "\n",
    "        return test_loss, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bored-salmon",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:31:23.770480Z",
     "iopub.status.busy": "2021-05-20T10:31:23.767835Z",
     "iopub.status.idle": "2021-05-20T10:31:23.771293Z",
     "shell.execute_reply": "2021-05-20T10:31:23.771918Z",
     "shell.execute_reply.started": "2021-05-20T10:25:34.512263Z"
    },
    "papermill": {
     "duration": 0.0318,
     "end_time": "2021-05-20T10:31:23.772098",
     "exception": false,
     "start_time": "2021-05-20T10:31:23.740298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    # general options\n",
    "    train_path = '../input/covid/train',         # train data folder\n",
    "    valid_path = '../input/covid/valid',         # valid data folder\n",
    "    test_path = '../input/covid/test',           # test data folder\n",
    "    batch_size = 32,                             # training and valid batch size\n",
    "    test_batch_size = 32,                        # batch size for testing\n",
    "    arc = 'VGG13',                               # VGG11, VGG13, VGG16, VGG19\n",
    "    epochs = 100,                                # maximum number of epochs to train\n",
    "    lr = 0.0001,                                 # learning rate\n",
    "    momentum = 0.9,                              # SGD momentum, for SGD only\n",
    "    optimizer = 'adam',                          # optimization method: sgd | adam\n",
    "    seed = 1234,                                 # random seed\n",
    "    log_interval = 5,                            # how many batches to wait before logging training status\n",
    "    patience = 5,                                # how many epochs of no loss improvement should we wait before stop training\n",
    "    checkpoint = '.',                            # checkpoints directory\n",
    "    train = True,                                # train before testing\n",
    "    cuda = True,                                 # use gpu\n",
    "\n",
    "    # feature extraction options\n",
    "    window_size = .04,                           # window size for the stft\n",
    "    window_stride = .02,                         # window stride for the stft\n",
    "    window_type = 'hamming',                     # window type for the stft\n",
    "    normalize = True,                            # use spect normalization\n",
    "    num_workers = 2,                             # how many subprocesses to use for data loading\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vietnamese-apollo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:31:23.897365Z",
     "iopub.status.busy": "2021-05-20T10:31:23.895509Z",
     "iopub.status.idle": "2021-05-20T10:31:28.804723Z",
     "shell.execute_reply": "2021-05-20T10:31:28.803799Z",
     "shell.execute_reply.started": "2021-05-20T10:25:34.523079Z"
    },
    "papermill": {
     "duration": 5.011392,
     "end_time": "2021-05-20T10:31:28.804864",
     "exception": false,
     "start_time": "2021-05-20T10:31:23.793472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA with 1 GPUs\n"
     ]
    }
   ],
   "source": [
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    print('Using CUDA with {0} GPUs'.format(torch.cuda.device_count()))\n",
    "\n",
    "\n",
    "# build model\n",
    "model = VGG(args.arc)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# Define criterion\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='mean') # This loss combines a Sigmoid layer and the BCELoss in one single class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-ecology",
   "metadata": {
    "papermill": {
     "duration": 0.011388,
     "end_time": "2021-05-20T10:31:28.828448",
     "exception": false,
     "start_time": "2021-05-20T10:31:28.817060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dominant-population",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:31:28.863486Z",
     "iopub.status.busy": "2021-05-20T10:31:28.862893Z",
     "iopub.status.idle": "2021-05-20T10:44:57.951148Z",
     "shell.execute_reply": "2021-05-20T10:44:57.950339Z"
    },
    "papermill": {
     "duration": 809.111214,
     "end_time": "2021-05-20T10:44:57.951283",
     "exception": false,
     "start_time": "2021-05-20T10:31:28.840069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [32/2160 (1%)]\tLoss: 0.692940\n",
      "Train Epoch: 1 [192/2160 (9%)]\tLoss: 0.693021\n",
      "Train Epoch: 1 [352/2160 (16%)]\tLoss: 0.693135\n",
      "Train Epoch: 1 [512/2160 (24%)]\tLoss: 0.692742\n",
      "Train Epoch: 1 [672/2160 (31%)]\tLoss: 0.692705\n",
      "Train Epoch: 1 [832/2160 (39%)]\tLoss: 0.692613\n",
      "Train Epoch: 1 [992/2160 (46%)]\tLoss: 0.692540\n",
      "Train Epoch: 1 [1152/2160 (53%)]\tLoss: 0.692520\n",
      "Train Epoch: 1 [1312/2160 (61%)]\tLoss: 0.692795\n",
      "Train Epoch: 1 [1472/2160 (68%)]\tLoss: 0.692748\n",
      "Train Epoch: 1 [1632/2160 (76%)]\tLoss: 0.692740\n",
      "Train Epoch: 1 [1792/2160 (83%)]\tLoss: 0.692135\n",
      "Train Epoch: 1 [1952/2160 (90%)]\tLoss: 0.692509\n",
      "Train Epoch: 1 [2112/2160 (98%)]\tLoss: 0.691998\n",
      "\n",
      "Validation set: Average loss: 0.6900, AUC: (61.0%)\n",
      "\n",
      "Saving state\n",
      "Elapsed seconds: (67s)\n",
      "Train Epoch: 2 [32/2160 (1%)]\tLoss: 0.687474\n",
      "Train Epoch: 2 [192/2160 (9%)]\tLoss: 0.687831\n",
      "Train Epoch: 2 [352/2160 (16%)]\tLoss: 0.691497\n",
      "Train Epoch: 2 [512/2160 (24%)]\tLoss: 0.688524\n",
      "Train Epoch: 2 [672/2160 (31%)]\tLoss: 0.687683\n",
      "Train Epoch: 2 [832/2160 (39%)]\tLoss: 0.687783\n",
      "Train Epoch: 2 [992/2160 (46%)]\tLoss: 0.687280\n",
      "Train Epoch: 2 [1152/2160 (53%)]\tLoss: 0.689333\n",
      "Train Epoch: 2 [1312/2160 (61%)]\tLoss: 0.688343\n",
      "Train Epoch: 2 [1472/2160 (68%)]\tLoss: 0.688599\n",
      "Train Epoch: 2 [1632/2160 (76%)]\tLoss: 0.689523\n",
      "Train Epoch: 2 [1792/2160 (83%)]\tLoss: 0.688990\n",
      "Train Epoch: 2 [1952/2160 (90%)]\tLoss: 0.689407\n",
      "Train Epoch: 2 [2112/2160 (98%)]\tLoss: 0.688826\n",
      "\n",
      "Validation set: Average loss: 0.6852, AUC: (64.0%)\n",
      "\n",
      "Saving state\n",
      "Elapsed seconds: (131s)\n",
      "Train Epoch: 3 [32/2160 (1%)]\tLoss: 0.671406\n",
      "Train Epoch: 3 [192/2160 (9%)]\tLoss: 0.679401\n",
      "Train Epoch: 3 [352/2160 (16%)]\tLoss: 0.679308\n",
      "Train Epoch: 3 [512/2160 (24%)]\tLoss: 0.678467\n",
      "Train Epoch: 3 [672/2160 (31%)]\tLoss: 0.682635\n",
      "Train Epoch: 3 [832/2160 (39%)]\tLoss: 0.681060\n",
      "Train Epoch: 3 [992/2160 (46%)]\tLoss: 0.678921\n",
      "Train Epoch: 3 [1152/2160 (53%)]\tLoss: 0.678960\n",
      "Train Epoch: 3 [1312/2160 (61%)]\tLoss: 0.680909\n",
      "Train Epoch: 3 [1472/2160 (68%)]\tLoss: 0.679136\n",
      "Train Epoch: 3 [1632/2160 (76%)]\tLoss: 0.677021\n",
      "Train Epoch: 3 [1792/2160 (83%)]\tLoss: 0.678754\n",
      "Train Epoch: 3 [1952/2160 (90%)]\tLoss: 0.678569\n",
      "Train Epoch: 3 [2112/2160 (98%)]\tLoss: 0.678173\n",
      "\n",
      "Validation set: Average loss: 0.6694, AUC: (64.4%)\n",
      "\n",
      "Saving state\n",
      "Elapsed seconds: (192s)\n",
      "Train Epoch: 4 [32/2160 (1%)]\tLoss: 0.698799\n",
      "Train Epoch: 4 [192/2160 (9%)]\tLoss: 0.645726\n",
      "Train Epoch: 4 [352/2160 (16%)]\tLoss: 0.656489\n",
      "Train Epoch: 4 [512/2160 (24%)]\tLoss: 0.659085\n",
      "Train Epoch: 4 [672/2160 (31%)]\tLoss: 0.658998\n",
      "Train Epoch: 4 [832/2160 (39%)]\tLoss: 0.659342\n",
      "Train Epoch: 4 [992/2160 (46%)]\tLoss: 0.661950\n",
      "Train Epoch: 4 [1152/2160 (53%)]\tLoss: 0.658065\n",
      "Train Epoch: 4 [1312/2160 (61%)]\tLoss: 0.658362\n",
      "Train Epoch: 4 [1472/2160 (68%)]\tLoss: 0.657321\n",
      "Train Epoch: 4 [1632/2160 (76%)]\tLoss: 0.656842\n",
      "Train Epoch: 4 [1792/2160 (83%)]\tLoss: 0.657103\n",
      "Train Epoch: 4 [1952/2160 (90%)]\tLoss: 0.656834\n",
      "Train Epoch: 4 [2112/2160 (98%)]\tLoss: 0.655910\n",
      "\n",
      "Validation set: Average loss: 0.6898, AUC: (63.0%)\n",
      "\n",
      "AUC was not improved, iteration 1\n",
      "Elapsed seconds: (255s)\n",
      "Train Epoch: 5 [32/2160 (1%)]\tLoss: 0.663623\n",
      "Train Epoch: 5 [192/2160 (9%)]\tLoss: 0.669114\n",
      "Train Epoch: 5 [352/2160 (16%)]\tLoss: 0.666796\n",
      "Train Epoch: 5 [512/2160 (24%)]\tLoss: 0.665851\n",
      "Train Epoch: 5 [672/2160 (31%)]\tLoss: 0.670298\n",
      "Train Epoch: 5 [832/2160 (39%)]\tLoss: 0.666080\n",
      "Train Epoch: 5 [992/2160 (46%)]\tLoss: 0.660614\n",
      "Train Epoch: 5 [1152/2160 (53%)]\tLoss: 0.660838\n",
      "Train Epoch: 5 [1312/2160 (61%)]\tLoss: 0.659048\n",
      "Train Epoch: 5 [1472/2160 (68%)]\tLoss: 0.654148\n",
      "Train Epoch: 5 [1632/2160 (76%)]\tLoss: 0.649897\n",
      "Train Epoch: 5 [1792/2160 (83%)]\tLoss: 0.651849\n",
      "Train Epoch: 5 [1952/2160 (90%)]\tLoss: 0.648903\n",
      "Train Epoch: 5 [2112/2160 (98%)]\tLoss: 0.649096\n",
      "\n",
      "Validation set: Average loss: 0.6669, AUC: (66.1%)\n",
      "\n",
      "Saving state\n",
      "Elapsed seconds: (315s)\n",
      "Train Epoch: 6 [32/2160 (1%)]\tLoss: 0.620860\n",
      "Train Epoch: 6 [192/2160 (9%)]\tLoss: 0.656582\n",
      "Train Epoch: 6 [352/2160 (16%)]\tLoss: 0.646915\n",
      "Train Epoch: 6 [512/2160 (24%)]\tLoss: 0.647643\n",
      "Train Epoch: 6 [672/2160 (31%)]\tLoss: 0.646524\n",
      "Train Epoch: 6 [832/2160 (39%)]\tLoss: 0.635932\n",
      "Train Epoch: 6 [992/2160 (46%)]\tLoss: 0.630001\n",
      "Train Epoch: 6 [1152/2160 (53%)]\tLoss: 0.626126\n",
      "Train Epoch: 6 [1312/2160 (61%)]\tLoss: 0.622514\n",
      "Train Epoch: 6 [1472/2160 (68%)]\tLoss: 0.621734\n",
      "Train Epoch: 6 [1632/2160 (76%)]\tLoss: 0.626502\n",
      "Train Epoch: 6 [1792/2160 (83%)]\tLoss: 0.629545\n",
      "Train Epoch: 6 [1952/2160 (90%)]\tLoss: 0.628871\n",
      "Train Epoch: 6 [2112/2160 (98%)]\tLoss: 0.629745\n",
      "\n",
      "Validation set: Average loss: 0.6806, AUC: (67.1%)\n",
      "\n",
      "Saving state\n",
      "Elapsed seconds: (377s)\n",
      "Train Epoch: 7 [32/2160 (1%)]\tLoss: 0.561015\n",
      "Train Epoch: 7 [192/2160 (9%)]\tLoss: 0.586453\n",
      "Train Epoch: 7 [352/2160 (16%)]\tLoss: 0.586383\n",
      "Train Epoch: 7 [512/2160 (24%)]\tLoss: 0.580895\n",
      "Train Epoch: 7 [672/2160 (31%)]\tLoss: 0.588245\n",
      "Train Epoch: 7 [832/2160 (39%)]\tLoss: 0.584126\n",
      "Train Epoch: 7 [992/2160 (46%)]\tLoss: 0.579670\n",
      "Train Epoch: 7 [1152/2160 (53%)]\tLoss: 0.588139\n",
      "Train Epoch: 7 [1312/2160 (61%)]\tLoss: 0.588468\n",
      "Train Epoch: 7 [1472/2160 (68%)]\tLoss: 0.593185\n",
      "Train Epoch: 7 [1632/2160 (76%)]\tLoss: 0.595115\n",
      "Train Epoch: 7 [1792/2160 (83%)]\tLoss: 0.595115\n",
      "Train Epoch: 7 [1952/2160 (90%)]\tLoss: 0.593977\n",
      "Train Epoch: 7 [2112/2160 (98%)]\tLoss: 0.594131\n",
      "\n",
      "Validation set: Average loss: 0.6614, AUC: (66.5%)\n",
      "\n",
      "AUC was not improved, iteration 1\n",
      "Elapsed seconds: (439s)\n",
      "Train Epoch: 8 [32/2160 (1%)]\tLoss: 0.579526\n",
      "Train Epoch: 8 [192/2160 (9%)]\tLoss: 0.536440\n",
      "Train Epoch: 8 [352/2160 (16%)]\tLoss: 0.524517\n",
      "Train Epoch: 8 [512/2160 (24%)]\tLoss: 0.531187\n",
      "Train Epoch: 8 [672/2160 (31%)]\tLoss: 0.523053\n",
      "Train Epoch: 8 [832/2160 (39%)]\tLoss: 0.534029\n",
      "Train Epoch: 8 [992/2160 (46%)]\tLoss: 0.540785\n",
      "Train Epoch: 8 [1152/2160 (53%)]\tLoss: 0.564756\n",
      "Train Epoch: 8 [1312/2160 (61%)]\tLoss: 0.575440\n",
      "Train Epoch: 8 [1472/2160 (68%)]\tLoss: 0.580846\n",
      "Train Epoch: 8 [1632/2160 (76%)]\tLoss: 0.585792\n",
      "Train Epoch: 8 [1792/2160 (83%)]\tLoss: 0.583858\n",
      "Train Epoch: 8 [1952/2160 (90%)]\tLoss: 0.586461\n",
      "Train Epoch: 8 [2112/2160 (98%)]\tLoss: 0.586144\n",
      "\n",
      "Validation set: Average loss: 0.6576, AUC: (67.3%)\n",
      "\n",
      "Saving state\n",
      "Elapsed seconds: (500s)\n",
      "Train Epoch: 9 [32/2160 (1%)]\tLoss: 0.486425\n",
      "Train Epoch: 9 [192/2160 (9%)]\tLoss: 0.555339\n",
      "Train Epoch: 9 [352/2160 (16%)]\tLoss: 0.513918\n",
      "Train Epoch: 9 [512/2160 (24%)]\tLoss: 0.511796\n",
      "Train Epoch: 9 [672/2160 (31%)]\tLoss: 0.522759\n",
      "Train Epoch: 9 [832/2160 (39%)]\tLoss: 0.519817\n",
      "Train Epoch: 9 [992/2160 (46%)]\tLoss: 0.530947\n",
      "Train Epoch: 9 [1152/2160 (53%)]\tLoss: 0.522198\n",
      "Train Epoch: 9 [1312/2160 (61%)]\tLoss: 0.517988\n",
      "Train Epoch: 9 [1472/2160 (68%)]\tLoss: 0.524123\n",
      "Train Epoch: 9 [1632/2160 (76%)]\tLoss: 0.517983\n",
      "Train Epoch: 9 [1792/2160 (83%)]\tLoss: 0.516840\n",
      "Train Epoch: 9 [1952/2160 (90%)]\tLoss: 0.518370\n",
      "Train Epoch: 9 [2112/2160 (98%)]\tLoss: 0.525810\n",
      "\n",
      "Validation set: Average loss: 0.7156, AUC: (63.5%)\n",
      "\n",
      "AUC was not improved, iteration 1\n",
      "Elapsed seconds: (562s)\n",
      "Train Epoch: 10 [32/2160 (1%)]\tLoss: 0.689886\n",
      "Train Epoch: 10 [192/2160 (9%)]\tLoss: 0.595654\n",
      "Train Epoch: 10 [352/2160 (16%)]\tLoss: 0.566257\n",
      "Train Epoch: 10 [512/2160 (24%)]\tLoss: 0.563074\n",
      "Train Epoch: 10 [672/2160 (31%)]\tLoss: 0.552662\n",
      "Train Epoch: 10 [832/2160 (39%)]\tLoss: 0.542966\n",
      "Train Epoch: 10 [992/2160 (46%)]\tLoss: 0.538365\n",
      "Train Epoch: 10 [1152/2160 (53%)]\tLoss: 0.530672\n",
      "Train Epoch: 10 [1312/2160 (61%)]\tLoss: 0.518773\n",
      "Train Epoch: 10 [1472/2160 (68%)]\tLoss: 0.509750\n",
      "Train Epoch: 10 [1632/2160 (76%)]\tLoss: 0.510615\n",
      "Train Epoch: 10 [1792/2160 (83%)]\tLoss: 0.505780\n",
      "Train Epoch: 10 [1952/2160 (90%)]\tLoss: 0.501523\n",
      "Train Epoch: 10 [2112/2160 (98%)]\tLoss: 0.493545\n",
      "\n",
      "Validation set: Average loss: 0.9092, AUC: (66.4%)\n",
      "\n",
      "AUC was not improved, iteration 2\n",
      "Elapsed seconds: (625s)\n",
      "Train Epoch: 11 [32/2160 (1%)]\tLoss: 0.318452\n",
      "Train Epoch: 11 [192/2160 (9%)]\tLoss: 0.380606\n",
      "Train Epoch: 11 [352/2160 (16%)]\tLoss: 0.401958\n",
      "Train Epoch: 11 [512/2160 (24%)]\tLoss: 0.410194\n",
      "Train Epoch: 11 [672/2160 (31%)]\tLoss: 0.423455\n",
      "Train Epoch: 11 [832/2160 (39%)]\tLoss: 0.426671\n",
      "Train Epoch: 11 [992/2160 (46%)]\tLoss: 0.427439\n",
      "Train Epoch: 11 [1152/2160 (53%)]\tLoss: 0.431935\n",
      "Train Epoch: 11 [1312/2160 (61%)]\tLoss: 0.433284\n",
      "Train Epoch: 11 [1472/2160 (68%)]\tLoss: 0.432811\n",
      "Train Epoch: 11 [1632/2160 (76%)]\tLoss: 0.428141\n",
      "Train Epoch: 11 [1792/2160 (83%)]\tLoss: 0.422615\n",
      "Train Epoch: 11 [1952/2160 (90%)]\tLoss: 0.425415\n",
      "Train Epoch: 11 [2112/2160 (98%)]\tLoss: 0.425537\n",
      "\n",
      "Validation set: Average loss: 1.3312, AUC: (39.1%)\n",
      "\n",
      "AUC was not improved, iteration 3\n",
      "Elapsed seconds: (686s)\n",
      "Train Epoch: 12 [32/2160 (1%)]\tLoss: 0.347860\n",
      "Train Epoch: 12 [192/2160 (9%)]\tLoss: 0.369722\n",
      "Train Epoch: 12 [352/2160 (16%)]\tLoss: 0.349409\n",
      "Train Epoch: 12 [512/2160 (24%)]\tLoss: 0.350681\n",
      "Train Epoch: 12 [672/2160 (31%)]\tLoss: 0.363364\n",
      "Train Epoch: 12 [832/2160 (39%)]\tLoss: 0.368395\n",
      "Train Epoch: 12 [992/2160 (46%)]\tLoss: 0.378292\n",
      "Train Epoch: 12 [1152/2160 (53%)]\tLoss: 0.398189\n",
      "Train Epoch: 12 [1312/2160 (61%)]\tLoss: 0.396637\n",
      "Train Epoch: 12 [1472/2160 (68%)]\tLoss: 0.394447\n",
      "Train Epoch: 12 [1632/2160 (76%)]\tLoss: 0.389685\n",
      "Train Epoch: 12 [1792/2160 (83%)]\tLoss: 0.383875\n",
      "Train Epoch: 12 [1952/2160 (90%)]\tLoss: 0.379776\n",
      "Train Epoch: 12 [2112/2160 (98%)]\tLoss: 0.372151\n",
      "\n",
      "Validation set: Average loss: 1.1057, AUC: (66.5%)\n",
      "\n",
      "AUC was not improved, iteration 4\n",
      "Elapsed seconds: (747s)\n",
      "Train Epoch: 13 [32/2160 (1%)]\tLoss: 0.287454\n",
      "Train Epoch: 13 [192/2160 (9%)]\tLoss: 0.325877\n",
      "Train Epoch: 13 [352/2160 (16%)]\tLoss: 0.367565\n",
      "Train Epoch: 13 [512/2160 (24%)]\tLoss: 0.336093\n",
      "Train Epoch: 13 [672/2160 (31%)]\tLoss: 0.332688\n",
      "Train Epoch: 13 [832/2160 (39%)]\tLoss: 0.314812\n",
      "Train Epoch: 13 [992/2160 (46%)]\tLoss: 0.304224\n",
      "Train Epoch: 13 [1152/2160 (53%)]\tLoss: 0.296127\n",
      "Train Epoch: 13 [1312/2160 (61%)]\tLoss: 0.288650\n",
      "Train Epoch: 13 [1472/2160 (68%)]\tLoss: 0.290329\n",
      "Train Epoch: 13 [1632/2160 (76%)]\tLoss: 0.285095\n",
      "Train Epoch: 13 [1792/2160 (83%)]\tLoss: 0.290124\n",
      "Train Epoch: 13 [1952/2160 (90%)]\tLoss: 0.289550\n",
      "Train Epoch: 13 [2112/2160 (98%)]\tLoss: 0.296263\n",
      "\n",
      "Validation set: Average loss: 1.5524, AUC: (45.8%)\n",
      "\n",
      "AUC was not improved, iteration 5\n",
      "Elapsed seconds: (809s)\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "if args.train:\n",
    "    train_dataset = Loader(args.train_path, window_size=args.window_size, window_stride=args.window_stride,\n",
    "        window_type=args.window_type, normalize=args.normalize)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "        num_workers=args.num_workers, pin_memory=args.cuda, sampler=None)\n",
    "\n",
    "    valid_dataset = Loader(args.valid_path, window_size=args.window_size, window_stride=args.window_stride,\n",
    "        window_type=args.window_type, normalize=args.normalize)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=args.batch_size, shuffle=None,\n",
    "        num_workers=args.num_workers, pin_memory=args.cuda, sampler=None)\n",
    "\n",
    "    # define optimizer\n",
    "    if args.optimizer.lower() == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    best_valid_auc = 0\n",
    "    iteration = 0\n",
    "    epoch = 1\n",
    "\n",
    "    # trainint with early stopping\n",
    "    t0 = time.time()\n",
    "    while (epoch < args.epochs + 1) and (iteration < args.patience):\n",
    "        train(train_loader, model, criterion, optimizer, epoch, args.cuda, args.log_interval,\n",
    "            weight=train_dataset.weight)\n",
    "        valid_loss, valid_auc = test(valid_loader, model, criterion, args.cuda, data_set='Validation')\n",
    "        if not os.path.isdir(args.checkpoint):\n",
    "            os.mkdir(args.checkpoint)\n",
    "        torch.save(model.state_dict(), './{}/model{:03d}.pt'.format(args.checkpoint, epoch))\n",
    "        if valid_auc <= best_valid_auc:\n",
    "            iteration += 1\n",
    "            print('AUC was not improved, iteration {0}'.format(str(iteration)))\n",
    "        else:\n",
    "            print('Saving state')\n",
    "            iteration = 0\n",
    "            best_valid_auc = valid_auc\n",
    "            state = {\n",
    "                'valid_auc': valid_auc,\n",
    "                'valid_loss': valid_loss,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            if not os.path.isdir(args.checkpoint):\n",
    "                os.mkdir(args.checkpoint)\n",
    "            torch.save(state, './{}/ckpt.pt'.format(args.checkpoint))\n",
    "        epoch += 1\n",
    "        print(f'Elapsed seconds: ({time.time() - t0:.0f}s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-oracle",
   "metadata": {
    "papermill": {
     "duration": 0.061113,
     "end_time": "2021-05-20T10:44:58.073043",
     "exception": false,
     "start_time": "2021-05-20T10:44:58.011930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bigger-sally",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T10:44:58.201536Z",
     "iopub.status.busy": "2021-05-20T10:44:58.200959Z",
     "iopub.status.idle": "2021-05-20T10:45:09.225408Z",
     "shell.execute_reply": "2021-05-20T10:45:09.224463Z"
    },
    "papermill": {
     "duration": 11.091513,
     "end_time": "2021-05-20T10:45:09.225652",
     "exception": false,
     "start_time": "2021-05-20T10:44:58.134139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model (epoch 8)\n",
      "Saving results in submission.csv\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Loader(args.test_path, window_size=args.window_size, window_stride=args.window_stride,\n",
    "    window_type=args.window_type, normalize=args.normalize)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=args.test_batch_size, shuffle=None,\n",
    "    num_workers=args.num_workers, pin_memory=args.cuda, sampler=None)\n",
    "\n",
    "# get best epoch\n",
    "state = torch.load('./{}/ckpt.pt'.format(args.checkpoint))\n",
    "epoch = state['epoch']\n",
    "print(\"Testing model (epoch {})\".format(epoch))\n",
    "model.load_state_dict(torch.load('./{}/model{:03d}.pt'.format(args.checkpoint, epoch)))\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "results = 'submission.csv'\n",
    "print(\"Saving results in {}\".format(results))\n",
    "test(test_loader, model, criterion, args.cuda, save=results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 837.033327,
   "end_time": "2021-05-20T10:45:11.028351",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-20T10:31:13.995024",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
